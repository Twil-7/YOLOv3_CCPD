训练过程中最神奇的是，必须先采用1e-3的学习率训练50轮，待到模型参数较佳时，再解开所有冰冻层，将学习率改为1e-4，才能取得这种最佳权重效果。

如果最开始就解冻所有层，将训练得到NAN。
如果不解冻，仅仅使用1e-3不断训练， val loss达到瓶颈时的效果很差，此后也无法再继续下降。

只有按上面这种训练方式，val loss才能降低，训练效果极为惊艳。
总共才不过3000张训练图片，val loss居然能降低到4.5左右，这在以前的实验中从未出现这么好的效果。然而如果仅按上面的方法训练，val loss达到瓶颈时最多只能降低到20附近，此时预测效果极差。

目前测试集的检测效果很惊艳，精度100%。且轮廓定位精准，几乎堪比YOLOv4。

......
# Epoch 28/50
# 90/90 [==============================] - 458s 5s/step - loss: 17.9293 - val_loss: 16.0469
# Epoch 29/50
# 90/90 [==============================] - 447s 5s/step - loss: 16.1895 - val_loss: 16.0067
# Epoch 30/50
# 90/90 [==============================] - 427s 5s/step - loss: 15.5724 - val_loss: 15.4911
# Epoch 31/50
# 90/90 [==============================] - 433s 5s/step - loss: 15.2257 - val_loss: 15.2047
# Epoch 32/50
# 90/90 [==============================] - 446s 5s/step - loss: 15.2845 - val_loss: 15.0738
# Epoch 33/50
# 90/90 [==============================] - 433s 5s/step - loss: 15.0068 - val_loss: 14.9617
# Epoch 34/50
# 90/90 [==============================] - 426s 5s/step - loss: 14.9663 - val_loss: 15.0940
# Epoch 35/50
# 90/90 [==============================] - 428s 5s/step - loss: 14.8779 - val_loss: 14.6620
# Epoch 36/50
# 90/90 [==============================] - 428s 5s/step - loss: 14.8770 - val_loss: 14.6631
# Epoch 37/50
# 90/90 [==============================] - 443s 5s/step - loss: 14.8071 - val_loss: 14.8948
# Epoch 38/50
# 90/90 [==============================] - 431s 5s/step - loss: 14.7455 - val_loss: 15.0602
# Epoch 39/50
# 90/90 [==============================] - 428s 5s/step - loss: 14.7603 - val_loss: 14.9556
# Epoch 40/50
# 90/90 [==============================] - 415s 5s/step - loss: 14.5794 - val_loss: 14.6628
# Epoch 41/50
# 90/90 [==============================] - 413s 5s/step - loss: 14.5958 - val_loss: 14.4788
# Epoch 42/50
# 90/90 [==============================] - 411s 5s/step - loss: 14.5451 - val_loss: 14.7405
# Epoch 43/50
# 90/90 [==============================] - 413s 5s/step - loss: 14.5774 - val_loss: 15.0327
# Epoch 44/50
# 90/90 [==============================] - 416s 5s/step - loss: 14.7166 - val_loss: 14.3137
# Epoch 45/50
# 90/90 [==============================] - 411s 5s/step - loss: 14.5778 - val_loss: 14.7716
# Epoch 46/50
# 90/90 [==============================] - 414s 5s/step - loss: 14.6534 - val_loss: 14.7974
# Epoch 47/50
# 90/90 [==============================] - 416s 5s/step - loss: 14.4697 - val_loss: 14.5442
# Epoch 48/50
# 90/90 [==============================] - 414s 5s/step - loss: 14.5342 - val_loss: 14.1271
# Epoch 49/50
# 90/90 [==============================] - 414s 5s/step - loss: 14.5800 - val_loss: 14.9744
# Epoch 50/50
# 90/90 [==============================] - 415s 5s/step - loss: 14.4249 - val_loss: 14.4055
# Epoch 51/100
# 90/90 [==============================] - 2517s 28s/step - loss: 13.6727 - val_loss: 12.8076
# Epoch 52/100
# 90/90 [==============================] - 2521s 28s/step - loss: 12.4827 - val_loss: 12.1194
# Epoch 53/100
# 90/90 [==============================] - 2505s 28s/step - loss: 12.2603 - val_loss: 12.1752
# Epoch 54/100
# 90/90 [==============================] - 2503s 28s/step - loss: 12.0118 - val_loss: 11.8849
# Epoch 55/100
# 90/90 [==============================] - 2489s 28s/step - loss: 12.0275 - val_loss: 11.7788
# Epoch 56/100
# 90/90 [==============================] - 2536s 28s/step - loss: 11.7137 - val_loss: 11.4785
# Epoch 57/100
# 90/90 [==============================] - 2508s 28s/step - loss: 11.5155 - val_loss: 11.3267
# Epoch 58/100
# 90/90 [==============================] - 2510s 28s/step - loss: 11.3421 - val_loss: 11.0188
# Epoch 59/100
# 90/90 [==============================] - 2496s 28s/step - loss: 11.2341 - val_loss: 10.8827
# Epoch 60/100
# 90/90 [==============================] - 2533s 28s/step - loss: 11.0715 - val_loss: 11.2347
# Epoch 61/100
# 90/90 [==============================] - 2531s 28s/step - loss: 10.8625 - val_loss: 10.7041
# Epoch 62/100
# 90/90 [==============================] - 2510s 28s/step - loss: 10.7207 - val_loss: 10.5346
# Epoch 63/100
# 90/90 [==============================] - 2539s 28s/step - loss: 10.5706 - val_loss: 10.4150
# Epoch 64/100
# 90/90 [==============================] - 2483s 28s/step - loss: 10.4031 - val_loss: 10.4083
# Epoch 65/100
# 90/90 [==============================] - 2464s 27s/step - loss: 10.1796 - val_loss: 10.2115
# Epoch 66/100
# 90/90 [==============================] - 2460s 27s/step - loss: 10.0471 - val_loss: 10.0037
# Epoch 67/100
# 90/90 [==============================] - 2490s 28s/step - loss: 9.8013 - val_loss: 9.8446
# Epoch 68/100
# 90/90 [==============================] - 2510s 28s/step - loss: 9.7331 - val_loss: 9.4238
# Epoch 69/100
# 90/90 [==============================] - 2508s 28s/step - loss: 9.5232 - val_loss: 9.3714
# Epoch 70/100
# 90/90 [==============================] - 2488s 28s/step - loss: 9.4123 - val_loss: 8.9422
# Epoch 71/100
# 90/90 [==============================] - 2491s 28s/step - loss: 9.2012 - val_loss: 9.1542
# Epoch 72/100
# 90/90 [==============================] - 2494s 28s/step - loss: 9.0916 - val_loss: 9.0591
# Epoch 73/100
# 90/90 [==============================] - 3322s 37s/step - loss: 8.8358 - val_loss: 8.9045
# Epoch 74/100
# 90/90 [==============================] - 4652s 52s/step - loss: 8.7340 - val_loss: 8.4010
# Epoch 75/100
# 90/90 [==============================] - 4639s 52s/step - loss: 8.5758 - val_loss: 8.1609
# Epoch 76/100
# 90/90 [==============================] - 4618s 51s/step - loss: 8.4156 - val_loss: 8.2948
# Epoch 77/100
# 90/90 [==============================] - 4629s 51s/step - loss: 8.2767 - val_loss: 8.3007
# Epoch 78/100
# 90/90 [==============================] - 4680s 52s/step - loss: 8.1095 - val_loss: 8.1437
# Epoch 79/100
# 90/90 [==============================] - 4640s 52s/step - loss: 8.0405 - val_loss: 7.8348
# Epoch 80/100
# 90/90 [==============================] - 4617s 51s/step - loss: 7.8930 - val_loss: 7.7147
# Epoch 81/100
# 90/90 [==============================] - 4743s 53s/step - loss: 7.7637 - val_loss: 7.8382
# Epoch 82/100
# 90/90 [==============================] - 5181s 58s/step - loss: 7.6338 - val_loss: 7.6799
# Epoch 83/100
# 90/90 [==============================] - 5406s 60s/step - loss: 7.6002 - val_loss: 7.3247
# Epoch 84/100
# 90/90 [==============================] - 5409s 60s/step - loss: 7.3296 - val_loss: 7.6033
# Epoch 85/100
# 90/90 [==============================] - 5430s 60s/step - loss: 7.3786 - val_loss: 7.0158
# Epoch 86/100
# 90/90 [==============================] - 5340s 59s/step - loss: 7.1394 - val_loss: 6.9879
# Epoch 87/100
# 90/90 [==============================] - 5372s 60s/step - loss: 7.0430 - val_loss: 6.7655
# Epoch 88/100
# 90/90 [==============================] - 5340s 59s/step - loss: 6.8289 - val_loss: 6.6776
# Epoch 89/100
# 90/90 [==============================] - 5326s 59s/step - loss: 6.8103 - val_loss: 6.6894
# Epoch 90/100
# 90/90 [==============================] - 5499s 61s/step - loss: 6.6713 - val_loss: 6.5715
# Epoch 91/100
# 90/90 [==============================] - 5405s 60s/step - loss: 6.5768 - val_loss: 6.6105
# Epoch 92/100
# 90/90 [==============================] - 5456s 61s/step - loss: 6.4689 - val_loss: 6.3830
# Epoch 93/100
# 90/90 [==============================] - 5404s 60s/step - loss: 6.4997 - val_loss: 6.2773
# Epoch 94/100
# 90/90 [==============================] - 4840s 54s/step - loss: 6.2980 - val_loss: 6.1765
# Epoch 95/100
# 90/90 [==============================] - 4658s 52s/step - loss: 6.1925 - val_loss: 5.9884
# Epoch 96/150
# 90/90 [==============================] - 4679s 51s/step - loss: 6.1752 - val_loss: 5.9587
# Epoch 97/150
# 90/90 [==============================] - 4582s 51s/step - loss: 6.0597 - val_loss: 5.7917
# Epoch 98/150
# 90/90 [==============================] - 4601s 51s/step - loss: 5.9021 - val_loss: 5.7594
# Epoch 99/150
# 90/90 [==============================] - 4591s 51s/step - loss: 5.8363 - val_loss: 5.7323
# Epoch 100/150
# 90/90 [==============================] - 4658s 52s/step - loss: 5.6952 - val_loss: 5.4541
# Epoch 101/150
# 90/90 [==============================] - 4632s 51s/step - loss: 5.6003 - val_loss: 5.7198
# Epoch 102/150
# 90/90 [==============================] - 4630s 51s/step - loss: 5.5671 - val_loss: 5.2617
# Epoch 103/150
# 90/90 [==============================] - 4684s 52s/step - loss: 5.4389 - val_loss: 5.1549
# Epoch 104/150
# 90/90 [==============================] - 5137s 57s/step - loss: 5.3400 - val_loss: 5.1845
# Epoch 105/150
# 90/90 [==============================] - 5241s 58s/step - loss: 5.3546 - val_loss: 5.1286
# Epoch 106/150
# 90/90 [==============================] - 5185s 58s/step - loss: 5.3310 - val_loss: 5.0455
# Epoch 107/150
# 90/90 [==============================] - 5682s 63s/step - loss: 5.2411 - val_loss: 5.0670
# Epoch 108/150
# 90/90 [==============================] - 5973s 66s/step - loss: 5.0851 - val_loss: 5.0293
# Epoch 109/150
# 90/90 [==============================] - 5196s 58s/step - loss: 5.0246 - val_loss: 4.9559
# Epoch 110/150
# 90/90 [==============================] - 5187s 58s/step - loss: 5.0125 - val_loss: 4.7645
# Epoch 111/150
# 90/90 [==============================] - 5206s 58s/step - loss: 4.9703 - val_loss: 4.9506
# Epoch 112/150
# 90/90 [==============================] - 5281s 59s/step - loss: 4.8963 - val_loss: 4.9398
# Epoch 113/150
# 90/90 [==============================] - 5203s 58s/step - loss: 4.8718 - val_loss: 4.8429
#
# Epoch 00113: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.
# Epoch 114/150
# 90/90 [==============================] - 5254s 58s/step - loss: 4.7671 - val_loss: 4.6949
# Epoch 115/150
# 90/90 [==============================] - 5239s 58s/step - loss: 4.7074 - val_loss: 4.8951
# Epoch 116/150
# 90/90 [==============================] - 5170s 57s/step - loss: 4.7068 - val_loss: 4.6782
# Epoch 117/150
# 90/90 [==============================] - 5259s 58s/step - loss: 4.6952 - val_loss: 4.5572
# Epoch 118/150
# 90/90 [==============================] - 5214s 58s/step - loss: 4.7026 - val_loss: 4.6274
# Epoch 119/150
# 90/90 [==============================] - 5216s 58s/step - loss: 4.6862 - val_loss: 4.7420
# Epoch 120/150
# 90/90 [==============================] - 5182s 58s/step - loss: 4.6548 - val_loss: 4.8062
#
# Epoch 00120: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.
# Epoch 121/150
# 90/90 [==============================] - 5158s 57s/step - loss: 4.6372 - val_loss: 4.5779
# Epoch 122/150
# 90/90 [==============================] - 5238s 58s/step - loss: 4.6585 - val_loss: 4.5236
# Epoch 123/150
# 90/90 [==============================] - 5139s 57s/step - loss: 4.6496 - val_loss: 4.5462
# Epoch 124/150
# 90/90 [==============================] - 5194s 58s/step - loss: 4.6370 - val_loss: 4.6401
# Epoch 125/150
# 90/90 [==============================] - 5233s 58s/step - loss: 4.6011 - val_loss: 4.5362
#
# Epoch 00125: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.
# Epoch 126/150
# 90/90 [==============================] - 5172s 57s/step - loss: 4.6071 - val_loss: 4.5255
